# Dynamical Systems

Consider a deterministic, scalar system of a continous valued variable evolving in continous time $t$. 

i.e also have initial condition $x_0$ taken by $x$ at time $t_0$.

=> Last time we saw examples where $f()$ was linear.

An example of nonlinear dynamical system (modeling infection spread) where $x$ is the fraction of infected individuals: $$\frac{dx}{dt}=\beta x(1-x)$$ 

One can also consider dynamical systems of several variables (nodes of network).

$\frac{dx}{dt} = f(x,y), \frac{dy}{dt}=g(x,y)$ 

Can also have time-varying $\frac{dx}{dt}=f(x,t)$ 

In order to remove time from the system, we introduce $y=t$ and say

$\frac{dx}{dt}=f(x,y), \frac{dy}{dt}=1, y(0)=0$ 

Don't need time-varying systems, since we can always augment state space.

Another generalization:

Systems governed by higher-order derivatives:

$\frac{d^2x}{dt^2} + (\frac{dx}{dt})^2 - \frac{dx}{dt} = f(x)$ 

Introduce $y = \frac{dx}{dt}$ so $\frac{dx}{dt} =y, f(x)-y^+y$ 



Focus on first order dynamics that are not time-varying. Simply one-variable dynamics can always be solved in principle



$$\frac{dx}{dt}=f(x)$$ which means $\int_{x_0}^{x} \frac{dx`}{f(x`)} = t-t_0$. Howver, the integral might be difficult or unknown in the closed form. "Solving" means going from an implicit characterization of the system state to an explicit one. For two or more variables it's not generally possible to find a solution (networks with many variables).



### Fixed point analysis

A fixed point is a steady-state value of the system, so it remains stationary:

![Cosine Fixed Point](https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Cosine_fixed_point.svg/750px-Cosine_fixed_point.svg.png)

Fixed point $x = x^+$ is such that

$f(x^2) = 0$ or $\frac{dx}{dt}=0$ 

For two-variable systems, fixed point is pair $(x^s, y^s)$ such that:

$f(x^s, y^s)  = 0$ and $g(x^s, y^s) = 0$

Dynamics easier to undertand near fixed points

$x = x^* + \epsilon$ 

$\frac{dx}{dt} = \frac{dx^*}{dt}+\frac{d\epsilon}{dt}$

$\frac{dx}{dt}=\frac{d\epsilon}{dt} = f(x^* + \epsilon)$ 



Taylor series expansion about $x=x^*$

$\frac{d\epsilon}{dt} = f(x^*) + \epsilon f`(x^*) + O(\epsilon^2)$

Where $f`()$ is derivative

Neglecting high-order terms:

$\frac{d\epsilon}{dt} = \epsilon f`(x^*)$

First-order linear differential equation

$\epsilon(t) = \epsilon(0) e^{\lambda t}$ where $\lambda = f`(x^*)$ 

Depending of the sign of $\lambda$, distance $\epsilon$ from fixed point will either grow or decay exponentially in time => linear stability analysis



For $\lambda.< 0 $: Attracting fixed points: ie for which points close to fixed-point are attracted to it

For $\lambda > 0$: Repelling fixed point

For $\lambda = 0$: Either attracting or repelling based on higher order terms, other strange things.



### Extend Linear stablity analysis to Multivariate Systems

$(x^*, y^*)$ where $f(x^*, y^*) = 0$ and $g(x^*, y^*) = 0$

Consider nearby $x = x^* + \epsilon_x, y = y^* + \epsilon_y$

$\frac{dx}{dt} = \frac{d\epsilon_x}{dt} = f(x^* + \epsilon_x, y^* + \epsilon_y)$

$=\epsilon_x f^{(x)}(x^*, y^*) + \epsilon_yf^{(y)}(x^*, y^*)+...$ 

$\frac{d\epsilon_x}{dt} = \epsilon_x f^{(x)}(x^*, y^*) + \epsilon_yf^{(y)}(x^*, y^*)$

$\frac{d\epsilon_y}{dt} = \epsilon_x g^{(x)}(x^*, y^*) + \epsilon_yg^{(y)}(x^*, y^*)$



### Matrix-Vector Form

$\frac{d\epsilon}{dt} = J\epsilon$ Where 



